{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: <br>\n",
    "Data science basics<br>\n",
    "Doing Data Science<br>\n",
    "By: Cathy O'Neil; Rachel Schutt<br>\n",
    "Publisher: O'Reilly Media, Inc.<br>\n",
    "Pub. Date: October 24, 2013<br>\n",
    "Print ISBN-13: 978-1-4493-5865-5<br>\n",
    "Chapter 3 <br>\n",
    "http://proquest.safaribooksonline.com.proxy.lib.odu.edu/book/databases/9781449363871\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN\n",
    "\n",
    "\"K-NN is an algorithm that can be used when you have a bunch of objects that have been classified or labeled in some way, and other similar objects that havenâ€™t gotten classified or labeled yet, and you want a way to automatically label them.\"\n",
    "\n",
    "The key idea behind K-NN is that if we have a new object to be classified, we look at the distance of the new object, using some metric, to  all the already classified objects. Next sort these distances, and select the \"k\" closest (say k=5) objects to the new object. We look at how these 5 objects has been classified, and classify the new object based on that (typicall by voting of the 5 nearest neighbor). \n",
    "\n",
    "The two questions are: (a) what similarity/distance metric to use, and (b) what is a good value of k. The answer to both depends on the application data. The parameter k is also referred as a hyperparameter which requires tuning. \n",
    "\n",
    "We now first outline steps involved in a typical machine learning algorithm without any hyperparameter, and then we discuss a typical machine learning algorithm that requires hyperparameter tuning consists of the following steps.\n",
    "\n",
    "\n",
    "\n",
    "As such for this algorithm there is no learning involved, all the computation is happening when we are trying to classify a new object. However, most efficent implementation try to preprocess the esisting clasiffied data so that the query for k-nearest neighbor on a new object can be executed efficiently. \n",
    "\n",
    "\n",
    "We now first outline steps  in a typical machine learning algorithm without any hyper parameter.\n",
    "<ul>\n",
    "<li> Split the data into two sets: training set and test set. This could be 60/40, 80/20, etc.\n",
    "<li> Train/Fit  the model using training data.\n",
    "<li>  Evaluate the ML algorithm accuracy  on test data. This is a measure what you can expect the trained algorithm to perform on the new data.\n",
    "</ul>\n",
    "\n",
    "A machine learning algorithm with a hyper parameter consists of following steps.\n",
    "<ul>\n",
    "<li> Split the data into two sets: training set and test set. This could be 60/40, 80/20, etc.\n",
    "<li> Use training data to tune the hyper parameter\n",
    "       <ul>\n",
    "               <li> Split the training data into a a slightly smaller training set and validation set (80/20). When training data is small we can use cross-validation approach.\n",
    "               <li> Use the two sets to tune the hyper parameter.  That is, use different values of hyper parameter and evaluate the accuracy on the validation set for the ML algorithm that has been trained using the slightly smaller training set. The value of hyper parameter that results in the best accuracy is selected for further use.\n",
    "       </ul>\n",
    "<li> Train/Fit  the model using  the full training data and the value of hyper parameter selected above.\n",
    "\n",
    "<li>  Evaluate the ML algorithm accuracy on test data. This is a measure of accuracy what you can expect the trained algorithm to perform on the new data.\n",
    "\n",
    "</ul> \n",
    "\n",
    "\n",
    "For applying these steps to K-NN, we first need to decide on the similarity or distance metric. If the variables (features) have the same scale, a simple metric to use is Euclidean distance. In our example, we will use this metric. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x108377470>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAFgCAYAAAAM1fZgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5/8DE1J82YjsiwIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1083772b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(\"petal_length\", \"petal_width\", data=iris, hue=\"species\", fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IRIS dataset is a good candidate for K-NN approach. We will try this approach using all the four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(iris.pop('species')) # take species column out as a separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Euclidean distance \n",
    "irisa = iris.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.7  3.2  1.3  0.2]\n",
      "[ 5.   3.6  1.4  0.2]\n"
     ]
    }
   ],
   "source": [
    "# let us say we want to find Euclidean distance between row 2 and row 4\n",
    "p1 = irisa[2]\n",
    "p2 = irisa[4]\n",
    "print (p1)\n",
    "print (p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509901951359\n"
     ]
    }
   ],
   "source": [
    "dist = (p1[0]-p2[0])*(p1[0]-p2[0]) + (p1[1]-p2[1])*(p1[1]-p2[1]) + (p1[2]-p2[2])*(p1[2]-p2[2]) + (p1[3]-p2[3])*(p1[3]-p2[3])\n",
    "dist = np.sqrt(dist)\n",
    "print (dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.509901951359\n"
     ]
    }
   ],
   "source": [
    "dist = np.linalg.norm(p1-p2)\n",
    "print (dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(irisa, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(X_train))\n",
    "print (np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(y_train))\n",
    "print (np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "# Let us take first sample from the test set and see how we apply K-NN to determine its class\n",
    "pt = X_test[0]\n",
    "dist = np.linalg.norm(pt-X_train, axis = 1)\n",
    "print (np.shape(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use numpy argpartition to find k smallest elements in dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 15, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 4\n",
    "x = np.array([31, 40, 20, 11, 12, 15, 33, 43])\n",
    "ind = np.argpartition(x, k-1)[:k]\n",
    "x[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90 79 39 73 80]\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "minkind = np.argpartition(dist, k-1)[:k]\n",
    "print (minkind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3         0.2236068   0.43588989  0.50990195  0.50990195]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22360679774997896"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (dist[minkind])\n",
    "np.min(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor']\n"
     ]
    }
   ],
   "source": [
    "nn5 = y_train[minkind]\n",
    "print(nn5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the class that occurs most\n",
    "from collections import Counter\n",
    "ctnn5 = Counter(nn5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = ctnn5.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0] # prediction matches the test label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Repeat the above computation for predicting first test sample label for  IRIS data set with only two features petal_length and petal_width instead of using all the four features. Does the label for the first test sample matches with the true value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "versicolor virginica\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "irisN  = iris.drop(['sepal_length','sepal_width'],axis=1)\n",
    "irisaN = irisN.as_matrix()\n",
    "X_trainN, X_testN, y_trainN, y_testN = train_test_split(irisaN, y, test_size=0.2, random_state=0)\n",
    "pt=X_testN[0]\n",
    "dist=np.linalg.norm(pt-X_trainN,axis=1)\n",
    "k=5\n",
    "minkind = np.argpartition(dist,k-1)[:k]\n",
    "nn5 = y_train[minkind]\n",
    "ctnn5 = Counter(nn5)\n",
    "pred= ctnn5.most_common(1)[0][0]\n",
    "print(pred,y_testN[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do this in the loop for all the test data and compute the accuracy with k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "m = len(y_test)\n",
    "pred = np.empty(m, dtype=object)\n",
    "print(np.shape(pred))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-25412b0de784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mctnn5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctnn5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(m):\n",
    "    pt = X_test[i]\n",
    "    dist = np.linalg.norm(pt-X_train, axis = 1)\n",
    "    minkind = np.argpartition(dist, k-1)[:k]\n",
    "    nn5 = y_train[minkind]\n",
    "    ctnn5 = Counter(nn5)\n",
    "    pred[i] = ctnn5.most_common(1)[0][0]\n",
    "accuracy_score(y_test , pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Calculate the accuracy score for IRIS data set with only two features petal_length and petal_width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had picked value of k arbitrarily. How to tune it? Take the training data and split it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "X_train, y_train, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.shape(X_train1))\n",
    "print (np.shape(X_test1))\n",
    "print (np.shape(y_train1))\n",
    "print (np.shape(y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y_test1)\n",
    "pred = np.empty(m, dtype=object)\n",
    "k = 7 # you can easilty setup a loop for differnt k values\n",
    "for i in range(m):\n",
    "    pt = X_test1[i]\n",
    "    dist = np.linalg.norm(pt-X_train1, axis = 1)\n",
    "    minkind = np.argpartition(dist, k-1)[:k]\n",
    "    nn5 = y_train1[minkind]\n",
    "    ctnn5 = Counter(nn5)\n",
    "    print (ctnn5)\n",
    "    pred[i] = ctnn5.most_common(1)[0][0] \n",
    "accuracy_score(y_test1, pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-NN from Scikit Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
       "       'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.6,  1.4,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us take 1/3 for testing this time \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "irisa, y, test_size=0.33, random_state=42)#irisa as original data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "accuracy = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print (accuracy.mean())knn = KNeighborsClassifier(n_neighbors=k)\n",
    "accuracy = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print (accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_array = []\n",
    "k_array = []\n",
    "for k in range(3,50,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    accuracy = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    accuracy_array.append(accuracy.mean())\n",
    "    k_array.append(k)\n",
    "print (accuracy_array)\n",
    "print(k_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_error = 1.0 - np.array(accuracy_array)\n",
    "plt.plot(k_array, class_error)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ind = np.argmin(class_error)\n",
    "OptK = k_array[min_ind]\n",
    "print (\"Optimal value of K is %d \" %  OptK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Optimal value of K for test set\n",
    "knn = KNeighborsClassifier(n_neighbors=OptK)\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Load the following data set as shown below. Each row in X is a 64-dimensional data. Use K-NN from Scikit package to evaluate its effectiveness for this dataset. For this you need to partition the data into training and test set. Also, use the cross_val_score to tune the hyper parameter k.  Using the tuned k apply K-NN and evaluate the accuracy_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  dataset\n",
    "digits = datasets.load_digits()\n",
    "X3D = digits.images # loaded as 3d array (each row is a 8 x 8 image)\n",
    "n_samples = len(X3D)\n",
    "X = X3D.reshape((n_samples, -1)) # flatten the array \n",
    "print(np.shape(X))\n",
    "y = digits.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "accuracy = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print (accuracy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_array1 = []\n",
    "k_array1 = []\n",
    "for k in range(1,30,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    accuracy = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    accuracy_array1.append(accuracy.mean())\n",
    "    k_array1.append(k)\n",
    "print (\"here\",accuracy_array1)\n",
    "print(k_array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error1 = 1.0 - np.array(accuracy_array1)\n",
    "plt.plot(k_array1, class_error1)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Classification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ind1 = np.argmin(class_error1)\n",
    "OptK1 = k_array1[min_ind1]\n",
    "print (\"Optimal value of K is %d \" %  OptK1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Optimal value of K for test set\n",
    "knn = KNeighborsClassifier(n_neighbors=OptK1)\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(X, y)\n",
    "\n",
    "# predict\n",
    "pred = knn.predict(X)\n",
    "\n",
    "# evaluate accuracy\n",
    "print(accuracy_score(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
